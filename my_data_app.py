import streamlit as st
import pandas as pd
import sqlite3
from requests import get
from bs4 import BeautifulSoup as bs
import time
from io import BytesIO

# Configuration de la page
st.set_page_config(page_title="CoinAfrique Animal Scraper", layout="wide")

st.markdown("<h1 style='text-align: center; color: #FF6B35;'>üêæ CoinAfrique Animal Data App</h1>", unsafe_allow_html=True)

st.markdown("""
Cette application vous permet de :
* **Scraper des donn√©es** d'animaux sur plusieurs pages de CoinAfrique
* **T√©l√©charger des donn√©es** d√©j√† scrap√©es (non nettoy√©es)
* **Visualiser un dashboard** avec les donn√©es nettoy√©es
* **Remplir un formulaire** d'√©valuation de l'application

**Librairies Python:** pandas, streamlit, sqlite3, beautifulsoup4, requests
**Source de donn√©es:** [CoinAfrique S√©n√©gal](https://sn.coinafrique.com/)
""")

# Fonction pour cr√©er la base de donn√©es SQLite
def init_database():
    conn = sqlite3.connect('coinafrique_animals.db')
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS animals
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  category TEXT,
                  name TEXT,
                  price TEXT,
                  address TEXT,
                  image_url TEXT,
                  scrape_date TEXT)''')
    conn.commit()
    conn.close()

# Fonction de scraping
def scrape_category(base_url, category_name, max_pages=10):
    df = pd.DataFrame()
    
    for page in range(1, max_pages + 1):
        try:
            url = f"{base_url}?page={page}"
            res = get(url, timeout=10)
            soup = bs(res.content, 'html.parser')
            containers = soup.find_all('div', class_='col s6 m4 l3')
            
            if not containers:
                break
            
            data = []
            for container in containers:
                try:
                    name = container.find('p', 'ad__card-description').text.strip()
                    price = container.find('p', 'ad__card-price').text.replace('CFA', '').replace(' ', '').strip()
                    adresse = container.find('p', 'ad__card-location').span.text.strip()
                    image_url = container.find('img', class_='ad__card-img')['src']
                    
                    dic = {
                        'category': category_name,
                        'name': name,
                        'price': price,
                        'address': adresse,
                        'image_url': image_url,
                        'scrape_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    data.append(dic)
                except:
                    pass
            
            DF = pd.DataFrame(data)
            df = pd.concat([df, DF], axis=0).reset_index(drop=True)
            time.sleep(1)  # Pause entre les pages
            
        except Exception as e:
            st.warning(f"Erreur page {page}: {str(e)}")
            break
    
    return df

# Fonction pour nettoyer les donn√©es
def clean_data(df):
    df_clean = df.copy()
    
    # Nettoyer les prix
    df_clean['price_clean'] = df_clean['price'].str.replace(r'\D', '', regex=True)
    df_clean['price_clean'] = pd.to_numeric(df_clean['price_clean'], errors='coerce')
    
    # Supprimer les doublons
    df_clean = df_clean.drop_duplicates(subset=['name', 'address'])
    
    # Supprimer les lignes avec des valeurs manquantes importantes
    df_clean = df_clean.dropna(subset=['name', 'price_clean'])
    
    return df_clean

# Fonction pour sauvegarder dans SQLite
def save_to_database(df):
    conn = sqlite3.connect('coinafrique_animals.db')
    df.to_sql('animals', conn, if_exists='append', index=False)
    conn.close()

# Fonction pour charger depuis SQLite
def load_from_database():
    conn = sqlite3.connect('coinafrique_animals.db')
    df = pd.read_sql_query("SELECT * FROM animals", conn)
    conn.close()
    return df

# Initialiser la base de donn√©es
init_database()

# Sidebar pour la navigation
menu = st.sidebar.radio("Menu", ["üîç Scraper des donn√©es", "üì• T√©l√©charger donn√©es brutes", "üìä Dashboard", "üìù Formulaire d'√©valuation"])

# SECTION 1: Scraper des donn√©es
if menu == "üîç Scraper des donn√©es":
    st.header("Scraper des donn√©es CoinAfrique")
    
    categories = {
        "Chiens": "https://sn.coinafrique.com/categorie/chiens",
        "Moutons": "https://sn.coinafrique.com/categorie/moutons",
        "Poules, lapins et pigeons": "https://sn.coinafrique.com/categorie/poules-lapins-et-pigeons",
        "Autres animaux": "https://sn.coinafrique.com/categorie/autres-animaux"
    }
    
    selected_category = st.selectbox("S√©lectionner une cat√©gorie", list(categories.keys()))
    max_pages = st.slider("Nombre de pages √† scraper", 1, 20, 5)
    
    if st.button("üöÄ Lancer le scraping", key="scrape_btn"):
        with st.spinner(f"Scraping en cours pour {selected_category}..."):
            df_scraped = scrape_category(categories[selected_category], selected_category, max_pages)
            
            if not df_scraped.empty:
                st.success(f"‚úÖ {len(df_scraped)} annonces scrap√©es !")
                
                # Sauvegarder dans la base de donn√©es
                save_to_database(df_scraped)
                st.info("üíæ Donn√©es sauvegard√©es dans la base de donn√©es SQLite")
                
                # Afficher un aper√ßu
                st.subheader("Aper√ßu des donn√©es scrap√©es")
                st.dataframe(df_scraped)
            else:
                st.warning("Aucune donn√©e trouv√©e.")

# SECTION 2: T√©l√©charger donn√©es brutes
elif menu == "üì• T√©l√©charger donn√©es brutes":
    st.header("T√©l√©charger les donn√©es brutes (non nettoy√©es)")
    
    try:
        df_raw = load_from_database()
        
        if not df_raw.empty:
            st.write(f"**Total d'annonces:** {len(df_raw)}")
            st.write(f"**Dimensions:** {df_raw.shape[0]} lignes √ó {df_raw.shape[1]} colonnes")
            
            # Grouper par cat√©gorie
            category_counts = df_raw['category'].value_counts()
            st.write("**R√©partition par cat√©gorie:**")
            st.dataframe(category_counts)
            
            # Bouton de t√©l√©chargement
            csv = df_raw.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="üì• T√©l√©charger toutes les donn√©es (CSV)",
                data=csv,
                file_name='coinafrique_animals_raw.csv',
                mime='text/csv',
            )
            
            # Afficher les donn√©es
            if st.checkbox("Afficher toutes les donn√©es"):
                st.dataframe(df_raw)
        else:
            st.info("Aucune donn√©e disponible. Veuillez d'abord scraper des donn√©es.")
    except Exception as e:
        st.error(f"Erreur lors du chargement: {str(e)}")

# SECTION 3: Dashboard
elif menu == "üìä Dashboard":
    st.header("Dashboard des donn√©es nettoy√©es")
    
    try:
        df_raw = load_from_database()
        
        if not df_raw.empty:
            df_clean = clean_data(df_raw)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total d'annonces", len(df_clean))
            with col2:
                st.metric("Prix moyen", f"{df_clean['price_clean'].mean():,.0f} CFA")
            with col3:
                st.metric("Cat√©gories", df_clean['category'].nunique())
            
            st.subheader("üìà Statistiques par cat√©gorie")
            category_stats = df_clean.groupby('category').agg({
                'price_clean': ['count', 'mean', 'min', 'max']
            }).round(0)
            category_stats.columns = ['Nombre', 'Prix moyen', 'Prix min', 'Prix max']
            st.dataframe(category_stats)
            
            st.subheader("üí∞ Distribution des prix")
            st.bar_chart(df_clean.groupby('category')['price_clean'].mean())
            
            st.subheader("üìç R√©partition par localisation")
            top_locations = df_clean['address'].value_counts().head(10)
            st.bar_chart(top_locations)
            
            st.subheader("üîç Explorer les donn√©es nettoy√©es")
            st.dataframe(df_clean)
            
            # T√©l√©charger les donn√©es nettoy√©es
            csv_clean = df_clean.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="üì• T√©l√©charger les donn√©es nettoy√©es (CSV)",
                data=csv_clean,
                file_name='coinafrique_animals_clean.csv',
                mime='text/csv',
            )
        else:
            st.info("Aucune donn√©e disponible. Veuillez d'abord scraper des donn√©es.")
    except Exception as e:
        st.error(f"Erreur: {str(e)}")

# SECTION 4: Formulaire d'√©valuation
elif menu == "üìù Formulaire d'√©valuation":
    st.header("Formulaire d'√©valuation de l'application")
    
    with st.form("evaluation_form"):
        st.subheader("Votre avis compte !")
        
        nom = st.text_input("Nom (optionnel)")
        email = st.text_input("Email (optionnel)")
        
        rating = st.slider("Notez l'application", 1, 5, 3)
        
        ease_of_use = st.select_slider(
            "Facilit√© d'utilisation",
            options=["Tr√®s difficile", "Difficile", "Moyen", "Facile", "Tr√®s facile"]
        )
        
        features = st.multiselect(
            "Quelles fonctionnalit√©s avez-vous utilis√©es ?",
            ["Scraping", "T√©l√©chargement", "Dashboard", "Toutes"]
        )
        
        feedback = st.text_area("Commentaires et suggestions")
        
        submit = st.form_submit_button("Soumettre l'√©valuation")
        
        if submit:
            # Sauvegarder l'√©valuation
            evaluation_data = {
                'nom': nom,
                'email': email,
                'rating': rating,
                'ease_of_use': ease_of_use,
                'features': ', '.join(features),
                'feedback': feedback,
                'date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            conn = sqlite3.connect('coinafrique_animals.db')
            c = conn.cursor()
            c.execute('''CREATE TABLE IF NOT EXISTS evaluations
                         (id INTEGER PRIMARY KEY AUTOINCREMENT,
                          nom TEXT, email TEXT, rating INTEGER,
                          ease_of_use TEXT, features TEXT,
                          feedback TEXT, date TEXT)''')
            
            pd.DataFrame([evaluation_data]).to_sql('evaluations', conn, if_exists='append', index=False)
            conn.close()
            
            st.success("‚úÖ Merci pour votre √©valuation !")
            st.balloons()

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>D√©velopp√© avec ‚ù§Ô∏è par Streamlit | Donn√©es de CoinAfrique S√©n√©gal</p>
</div>
""", unsafe_allow_html=True)

